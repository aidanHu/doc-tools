import os
import re
import jieba
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import json
import markdown
from bs4 import BeautifulSoup
import yaml

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class MarkdownInfoDensityAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # ä¿¡æ¯ä»·å€¼æƒé‡
        self.weights = {
            'number': 3.0,      # æ•°å­—ä¿¡æ¯
            'time': 2.5,        # æ—¶é—´ä¿¡æ¯
            'entity': 2.0,      # å®ä½“ä¿¡æ¯
            'professional': 2.0, # ä¸“ä¸šæœ¯è¯­
            'code': 2.5,        # ä»£ç ç‰‡æ®µ
            'link': 1.5,        # é“¾æ¥
            'verb': 1.5,        # åŠ¨ä½œåŠ¨è¯
            'conjunction': 1.0,  # è¿æ¥è¯
            'modifier': -0.5,   # ä¿®é¥°è¯ï¼ˆé™æƒï¼‰
            'list_item': 1.2,   # åˆ—è¡¨é¡¹
            'heading': 1.8      # æ ‡é¢˜
        }
        
        # é¢„å®šä¹‰è¯æ±‡åº“
        self.time_keywords = [
            'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ç°åœ¨',
            'ç›®å‰', 'å½“å‰', 'è¿‘æœŸ', 'æœ€è¿‘', 'æœªæ¥', 'è¿‡å»', 'ä»¥å‰', 'ä¹‹å',
            '2023', '2024', '2025'
        ]
        
        self.conjunction_keywords = [
            'å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'åŒæ—¶', 'å¦å¤–', 'æ­¤å¤–', 'è€Œä¸”',
            'ä¸è¿‡', 'è™½ç„¶', 'å°½ç®¡', 'ç”±äº', 'é‰´äº', 'åŸºäº', 'é€šè¿‡', 'é¦–å…ˆ',
            'å…¶æ¬¡', 'æœ€å', 'æ€»ä¹‹', 'ç»¼ä¸Š', 'æ¢è¨€ä¹‹'
        ]
        
        self.modifier_keywords = [
            'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'ååˆ†', 'ç›¸å½“', 'æ¯”è¾ƒ', 'è¾ƒä¸º', 'æå…¶',
            'ç›¸å¯¹', 'åŸºæœ¬', 'å¤§æ¦‚', 'å¯èƒ½', 'æˆ–è®¸', 'ä¼¼ä¹', 'å¥½åƒ', 'æŒº',
            'è›®', 'è¿˜æ˜¯', 'æ¯”è¾ƒ'
        ]
        
        # ä¸“ä¸šæœ¯è¯­åº“ï¼ˆæŠ€æœ¯ç±»ï¼‰
        self.professional_terms = [
            'ç®—æ³•', 'æ•°æ®ç»“æ„', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'äººå·¥æ™ºèƒ½', 'ç¥ç»ç½‘ç»œ',
            'æ•°æ®åº“', 'æ¶æ„', 'æ¡†æ¶', 'API', 'æ¥å£', 'åè®®', 'æœåŠ¡å™¨', 'å®¢æˆ·ç«¯',
            'å‰ç«¯', 'åç«¯', 'å…¨æ ˆ', 'å¾®æœåŠ¡', 'å®¹å™¨', 'äº‘è®¡ç®—', 'å¤§æ•°æ®',
            'åŒºå—é“¾', 'ç‰©è”ç½‘', 'è¾¹ç¼˜è®¡ç®—', 'åˆ†å¸ƒå¼', 'å¹¶å‘', 'å¼‚æ­¥', 'åŒæ­¥',
            'ç¼“å­˜', 'ç´¢å¼•', 'ä¼˜åŒ–', 'æ€§èƒ½', 'å®‰å…¨', 'åŠ å¯†', 'è®¤è¯', 'æˆæƒ'
        ]
        
        # Markdownç‰¹æ®Šæ¨¡å¼
        self.md_patterns = {
            'code_block': r'```[\s\S]*?```',
            'inline_code': r'`[^`]+`',  # ä¸éœ€è¦æ•è·ç»„ï¼Œç›´æ¥ç§»é™¤
            'link': r'\[([^\]]+)\]\([^\)]+\)',
            'image': r'!\[([^\]]*)\]\([^\)]+\)',
            'heading': r'^#{1,6}\s+(.+)$',
            'list_item': r'^[\s]*[-*+]\s+(.+)$',
            'numbered_list': r'^[\s]*\d+\.\s+(.+)$',
            'blockquote': r'^>\s+(.+)$',
            'table_row': r'\|.*\|',
            'bold': r'\*\*([^*]+)\*\*',
            'italic': r'\*([^*]+)\*',
            'strikethrough': r'~~([^~]+)~~'
        }

    def parse_markdown_file(self, file_path):
        """è§£æMarkdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†ç¦»Front Matterå’Œæ­£æ–‡
            front_matter = {}
            body_content = content
            
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    try:
                        front_matter = yaml.safe_load(parts[1]) or {}
                        body_content = parts[2].strip()
                    except:
                        pass
            
            return body_content, front_matter, None
            
        except Exception as e:
            return None, {}, str(e)

    def extract_markdown_elements(self, content):
        """æå–Markdownå…ƒç´ """
        elements = {
            'headings': [],
            'code_blocks': [],
            'inline_codes': [],
            'links': [],
            'images': [],
            'list_items': [],
            'blockquotes': [],
            'tables': [],
            'emphasis': []
        }
        
        # æå–å„ç§å…ƒç´ 
        elements['headings'] = re.findall(self.md_patterns['heading'], content, re.MULTILINE)
        elements['code_blocks'] = re.findall(self.md_patterns['code_block'], content, re.DOTALL)
        elements['inline_codes'] = re.findall(self.md_patterns['inline_code'], content)
        elements['links'] = re.findall(self.md_patterns['link'], content)
        elements['images'] = re.findall(self.md_patterns['image'], content)
        elements['list_items'] = re.findall(self.md_patterns['list_item'], content, re.MULTILINE)
        elements['list_items'].extend(re.findall(self.md_patterns['numbered_list'], content, re.MULTILINE))
        elements['blockquotes'] = re.findall(self.md_patterns['blockquote'], content, re.MULTILINE)
        elements['tables'] = re.findall(self.md_patterns['table_row'], content, re.MULTILINE)
        
        # æå–å¼ºè°ƒæ–‡æœ¬
        elements['emphasis'].extend(re.findall(self.md_patterns['bold'], content))
        elements['emphasis'].extend(re.findall(self.md_patterns['italic'], content))
        
        return elements

    def clean_markdown_text(self, content):
        """æ¸…ç†Markdownæ–‡æœ¬ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
        # ç§»é™¤ä»£ç å—
        content = re.sub(self.md_patterns['code_block'], '', content, flags=re.DOTALL)
        
        # ç§»é™¤å†…è”ä»£ç ï¼ˆä¿®å¤ï¼šç›´æ¥ç§»é™¤ï¼Œä¸ä¿ç•™å†…å®¹ï¼‰
        content = re.sub(self.md_patterns['inline_code'], '', content)
        
        # å¤„ç†é“¾æ¥ï¼Œä¿ç•™é“¾æ¥æ–‡æœ¬
        content = re.sub(self.md_patterns['link'], r'\1', content)
        
        # ç§»é™¤å›¾ç‰‡
        content = re.sub(self.md_patterns['image'], '', content)
        
        # æ¸…ç†æ ‡é¢˜æ ‡è®°
        content = re.sub(r'^#{1,6}\s+', '', content, flags=re.MULTILINE)
        
        # æ¸…ç†åˆ—è¡¨æ ‡è®°
        content = re.sub(r'^[\s]*[-*+]\s+', '', content, flags=re.MULTILINE)
        content = re.sub(r'^[\s]*\d+\.\s+', '', content, flags=re.MULTILINE)
        
        # æ¸…ç†å¼•ç”¨æ ‡è®°
        content = re.sub(r'^>\s+', '', content, flags=re.MULTILINE)
        
        # æ¸…ç†å¼ºè°ƒæ ‡è®°
        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)
        content = re.sub(r'\*([^*]+)\*', r'\1', content)
        content = re.sub(r'~~([^~]+)~~', r'\1', content)
        
        # æ¸…ç†è¡¨æ ¼
        content = re.sub(r'\|.*\|', '', content)
        content = re.sub(r'^[-\s|:]+$', '', content, flags=re.MULTILINE)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+', ' ', content)
        
        return content.strip()


    def analyze_sentence(self, sentence, context=None):
        """åˆ†æå•å¥ä¿¡æ¯å¯†åº¦"""
        if not sentence.strip():
            return None
        
        # åˆ†è¯
        words = jieba.lcut(sentence)
        total_chars = len(sentence.replace(' ', ''))
        
        if total_chars == 0:
            return None
        
        # ç»Ÿè®¡å„ç±»ä¿¡æ¯
        stats = {
            'numbers': len(re.findall(r'\d+\.?\d*', sentence)),
            'time_words': sum(1 for word in words if any(t in word for t in self.time_keywords)),
            'professional_terms': sum(1 for word in words if word in self.professional_terms),
            'conjunctions': sum(1 for word in words if word in self.conjunction_keywords),
            'modifiers': sum(1 for word in words if word in self.modifier_keywords),
            'entities': len([w for w in words if len(w) > 1 and w.isalpha()]),
            'code_snippets': len(re.findall(r'`[^`]+`', sentence)),
            'links': len(re.findall(r'\[([^\]]+)\]\([^\)]+\)', sentence))
        }
        
        # ä¸Šä¸‹æ–‡åŠ åˆ†
        context_bonus = 0
        if context:
            if context.get('is_heading'):
                context_bonus += self.weights['heading']
            if context.get('is_list_item'):
                context_bonus += self.weights['list_item']
        
        # è®¡ç®—ä¿¡æ¯å¾—åˆ†
        info_score = (
            stats['numbers'] * self.weights['number'] +
            stats['time_words'] * self.weights['time'] +
            stats['professional_terms'] * self.weights['professional'] +
            stats['conjunctions'] * self.weights['conjunction'] +
            stats['modifiers'] * self.weights['modifier'] +
            stats['code_snippets'] * self.weights['code'] +
            stats['links'] * self.weights['link'] +
            context_bonus
        )
        
        # è®¡ç®—å¯†åº¦
        density = info_score / total_chars if total_chars > 0 else 0
        
        # è´¨é‡ç­‰çº§ï¼ˆé’ˆå¯¹Markdownè°ƒæ•´é˜ˆå€¼ï¼‰
        if density > 0.12:  # Markdownæ–‡æ¡£é˜ˆå€¼ç¨ä½
            quality = "é«˜"
        elif density > 0.06:
            quality = "ä¸­"
        else:
            quality = "ä½"
        
        return {
            'sentence': sentence[:150] + "..." if len(sentence) > 150 else sentence,
            'density': density,
            'quality': quality,
            'total_chars': total_chars,
            'info_score': info_score,
            'context': context or {},
            **stats
        }

    def analyze_markdown_content(self, content, filename=""):
        """åˆ†æMarkdownå†…å®¹"""
        if not content:
            return {
                'filename': filename,
                'error': 'å†…å®¹ä¸ºç©º',
                'total_sentences': 0,
                'average_density': 0,
                'high_quality_ratio': 0,
                'markdown_stats': {},
                'sentences': []
            }
        
        # æå–Markdownå…ƒç´ 
        md_elements = self.extract_markdown_elements(content)
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = self.clean_markdown_text(content)
        
        # åˆ†å¥åˆ†æ
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]+', clean_text)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]
        
        if not sentences:
            return {
                'filename': filename,
                'error': 'æœªæ‰¾åˆ°æœ‰æ•ˆå¥å­',
                'total_sentences': 0,
                'average_density': 0,
                'high_quality_ratio': 0,
                'markdown_stats': self.calculate_markdown_stats(md_elements, content),
                'sentences': []
            }
        
        # åˆ†ææ¯ä¸ªå¥å­
        sentence_results = []
        for sentence in sentences:
            # åˆ¤æ–­å¥å­ä¸Šä¸‹æ–‡
            context = self.get_sentence_context(sentence, content)
            result = self.analyze_sentence(sentence, context)
            if result:
                sentence_results.append(result)
        
        if not sentence_results:
            return {
                'filename': filename,
                'error': 'å¥å­åˆ†æå¤±è´¥',
                'total_sentences': 0,
                'average_density': 0,
                'high_quality_ratio': 0,
                'markdown_stats': self.calculate_markdown_stats(md_elements, content),
                'sentences': []
            }
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        densities = [r['density'] for r in sentence_results]
        average_density = sum(densities) / len(densities)
        high_quality_count = len([r for r in sentence_results if r['quality'] == 'é«˜'])
        high_quality_ratio = high_quality_count / len(sentence_results)
        
        return {
            'filename': filename,
            'total_sentences': len(sentence_results),
            'total_chars': sum(r['total_chars'] for r in sentence_results),
            'average_density': average_density,
            'high_quality_ratio': high_quality_ratio,
            'quality_distribution': {
                'é«˜': len([r for r in sentence_results if r['quality'] == 'é«˜']),
                'ä¸­': len([r for r in sentence_results if r['quality'] == 'ä¸­']),
                'ä½': len([r for r in sentence_results if r['quality'] == 'ä½'])
            },
            'markdown_stats': self.calculate_markdown_stats(md_elements, content),
            'content_stats': {
                'numbers_count': sum(r['numbers'] for r in sentence_results),
                'time_words_count': sum(r['time_words'] for r in sentence_results),
                'professional_terms_count': sum(r['professional_terms'] for r in sentence_results),
                'code_snippets_count': sum(r['code_snippets'] for r in sentence_results),
                'links_count': sum(r['links'] for r in sentence_results),
                'avg_sentence_length': sum(r['total_chars'] for r in sentence_results) / len(sentence_results)
            },
            'sentences': sentence_results[:15]  # ä¿ç•™å‰15ä¸ªå¥å­çš„è¯¦ç»†ä¿¡æ¯
        }

    def get_sentence_context(self, sentence, full_content):
        """è·å–å¥å­ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = {
            'is_heading': False,
            'is_list_item': False,
            'is_blockquote': False,
            'heading_level': 0
        }
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ ‡é¢˜ä¸­
        heading_matches = re.findall(r'^(#{1,6})\s+(.+)$', full_content, re.MULTILINE)
        for level, heading_text in heading_matches:
            if sentence in heading_text:
                context['is_heading'] = True
                context['heading_level'] = len(level)
                break
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆ—è¡¨ä¸­
        if re.search(r'^[\s]*[-*+]\s+.*' + re.escape(sentence[:20]), full_content, re.MULTILINE):
            context['is_list_item'] = True
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼•ç”¨ä¸­
        if re.search(r'^>\s+.*' + re.escape(sentence[:20]), full_content, re.MULTILINE):
            context['is_blockquote'] = True
        
        return context

    def calculate_markdown_stats(self, md_elements, content):
        """è®¡ç®—Markdownç‰¹æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        lines = content.split('\n')
        
        stats = {
            'headings_count': len(md_elements['headings']),
            'heading_levels': {},
            'code_blocks_count': len(md_elements['code_blocks']),
            'inline_codes_count': len(md_elements['inline_codes']),
            'links_count': len(md_elements['links']),
            'images_count': len(md_elements['images']),
            'list_items_count': len(md_elements['list_items']),
            'blockquotes_count': len(md_elements['blockquotes']),
            'tables_count': len([line for line in lines if '|' in line and line.strip().startswith('|')]),
            'emphasis_count': len(md_elements['emphasis']),
            'total_lines': len(lines),
            'empty_lines': len([line for line in lines if not line.strip()]),
            'content_lines': len([line for line in lines if line.strip()])
        }
        
        # ç»Ÿè®¡æ ‡é¢˜å±‚çº§åˆ†å¸ƒ
        for heading in md_elements['headings']:
            # ä»åŸæ–‡ä¸­æ‰¾åˆ°æ ‡é¢˜çš„å±‚çº§
            for line in lines:
                if heading in line and line.strip().startswith('#'):
                    level = len(line) - len(line.lstrip('#'))
                    stats['heading_levels'][f'h{level}'] = stats['heading_levels'].get(f'h{level}', 0) + 1
                    break
        
        # è®¡ç®—ç»“æ„åŒ–ç¨‹åº¦
        structured_elements = (stats['headings_count'] + stats['list_items_count'] + 
                             stats['tables_count'] + stats['blockquotes_count'])
        stats['structure_ratio'] = structured_elements / stats['content_lines'] if stats['content_lines'] > 0 else 0
        
        # è®¡ç®—ä»£ç å¯†åº¦
        code_elements = stats['code_blocks_count'] + stats['inline_codes_count']
        stats['code_density'] = code_elements / stats['content_lines'] if stats['content_lines'] > 0 else 0
        
        return stats

    def analyze_markdown_folder(self, folder_path, output_dir="markdown_analysis_results"):
        """æ‰¹é‡åˆ†æMarkdownæ–‡ä»¶å¤¹"""
        if not os.path.exists(folder_path):
            print(f"é”™è¯¯: æ–‡ä»¶å¤¹ {folder_path} ä¸å­˜åœ¨")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰Markdownæ–‡ä»¶
        md_extensions = ['.md', '.markdown', '.mdown', '.mkd']
        md_files = []
        
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in md_extensions):
                    md_files.append(os.path.join(root, file))
        
        if not md_files:
            print(f"è­¦å‘Š: åœ¨ {folder_path} ä¸­æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶ï¼Œå¼€å§‹åˆ†æ...")
        
        results = []
        failed_files = []
        
        for i, file_path in enumerate(md_files, 1):
            filename = os.path.relpath(file_path, folder_path)
            print(f"æ­£åœ¨å¤„ç† ({i}/{len(md_files)}): {filename}")
            
            # è§£ææ–‡ä»¶
            content, front_matter, error = self.parse_markdown_file(file_path)
            
            if error:
                failed_files.append({'filename': filename, 'error': error})
                continue
            
            # åˆ†æå†…å®¹
            result = self.analyze_markdown_content(content, filename)
            
            # æ·»åŠ Front Matterä¿¡æ¯
            result['front_matter'] = front_matter
            result['file_path'] = file_path
            
            results.append(result)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_reports(results, failed_files, output_dir)
        
        print(f"\nåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir} ç›®å½•ä¸­")
        print(f"æˆåŠŸåˆ†æ: {len(results)} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥æ–‡ä»¶: {len(failed_files)} ä¸ª")

    def generate_reports(self, results, failed_files, output_dir):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ç”ŸæˆExcelæŠ¥å‘Š
        self.generate_excel_report(results, output_dir, timestamp)
        
        # 2. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_visualizations(results, output_dir, timestamp)
        
        # 3. ç”ŸæˆJSONè¯¦ç»†æŠ¥å‘Š
        self.generate_json_report(results, failed_files, output_dir, timestamp)
        
        # 4. ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report(results, failed_files, output_dir, timestamp)

    def generate_excel_report(self, results, output_dir, timestamp):
        """ç”ŸæˆExcelæŠ¥å‘Š"""
        try:
            # æ–‡ä»¶æ±‡æ€»æ•°æ®
            summary_data = []
            for result in results:
                if 'error' not in result:
                    md_stats = result['markdown_stats']
                    content_stats = result['content_stats']
                    
                    summary_data.append({
                        'æ–‡ä»¶å': result['filename'],
                        'æ€»å¥æ•°': result['total_sentences'],
                        'æ€»å­—æ•°': result['total_chars'],
                        'å¹³å‡ä¿¡æ¯å¯†åº¦': round(result['average_density'], 4),
                        'é«˜è´¨é‡å¥å­å æ¯”': f"{result['high_quality_ratio']:.2%}",
                        'é«˜è´¨é‡å¥å­æ•°': result['quality_distribution']['é«˜'],
                        'ä¸­è´¨é‡å¥å­æ•°': result['quality_distribution']['ä¸­'],
                        'ä½è´¨é‡å¥å­æ•°': result['quality_distribution']['ä½'],
                        'æ ‡é¢˜æ•°é‡': md_stats['headings_count'],
                        'ä»£ç å—æ•°é‡': md_stats['code_blocks_count'],
                        'é“¾æ¥æ•°é‡': md_stats['links_count'],
                        'åˆ—è¡¨é¡¹æ•°é‡': md_stats['list_items_count'],
                        'å›¾ç‰‡æ•°é‡': md_stats['images_count'],
                        'ç»“æ„åŒ–ç¨‹åº¦': f"{md_stats['structure_ratio']:.2%}",
                        'ä»£ç å¯†åº¦': f"{md_stats['code_density']:.2%}",
                        'ä¸“ä¸šæœ¯è¯­æ•°é‡': content_stats['professional_terms_count'],
                        'æ•°å­—ä¿¡æ¯æ•°é‡': content_stats['numbers_count'],
                        'æ—¶é—´è¯æ•°é‡': content_stats['time_words_count']
                    })
            
            df_summary = pd.DataFrame(summary_data)
            
            # Markdownç»“æ„åˆ†æ
            structure_data = []
            for result in results:
                if 'error' not in result:
                    md_stats = result['markdown_stats']
                    structure_data.append({
                        'æ–‡ä»¶å': result['filename'],
                        'æ€»è¡Œæ•°': md_stats['total_lines'],
                        'å†…å®¹è¡Œæ•°': md_stats['content_lines'],
                        'ç©ºè¡Œæ•°': md_stats['empty_lines'],
                        'æ ‡é¢˜åˆ†å¸ƒ': str(md_stats['heading_levels']),
                        'ä»£ç å—': md_stats['code_blocks_count'],
                        'å†…è”ä»£ç ': md_stats['inline_codes_count'],
                        'è¡¨æ ¼æ•°é‡': md_stats['tables_count'],
                        'å¼•ç”¨æ•°é‡': md_stats['blockquotes_count'],
                        'å¼ºè°ƒæ–‡æœ¬': md_stats['emphasis_count']
                    })
            
            df_structure = pd.DataFrame(structure_data)
            
            # å¥å­è¯¦ç»†åˆ†æ
            detail_data = []
            for result in results:
                if 'error' not in result:
                    for sentence in result['sentences']:
                        detail_data.append({
                            'æ–‡ä»¶å': result['filename'],
                            'å¥å­å†…å®¹': sentence['sentence'],
                            'ä¿¡æ¯å¯†åº¦': round(sentence['density'], 4),
                            'è´¨é‡ç­‰çº§': sentence['quality'],
                            'å­—æ•°': sentence['total_chars'],
                            'æ•°å­—æ•°é‡': sentence['numbers'],
                            'æ—¶é—´è¯æ•°é‡': sentence['time_words'],
                            'ä¸“ä¸šæœ¯è¯­æ•°é‡': sentence['professional_terms'],
                            'ä»£ç ç‰‡æ®µæ•°é‡': sentence['code_snippets'],
                            'é“¾æ¥æ•°é‡': sentence['links'],
                            'æ˜¯å¦æ ‡é¢˜': sentence['context'].get('is_heading', False),
                            'æ˜¯å¦åˆ—è¡¨é¡¹': sentence['context'].get('is_list_item', False)
                        })
            
            df_detail = pd.DataFrame(detail_data)
            
            # ä¿å­˜åˆ°Excel
            excel_path = os.path.join(output_dir, f"Markdownåˆ†ææŠ¥å‘Š_{timestamp}.xlsx")
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df_summary.to_excel(writer, sheet_name='æ–‡ä»¶æ±‡æ€»', index=False)
                df_structure.to_excel(writer, sheet_name='ç»“æ„åˆ†æ', index=False)
                df_detail.to_excel(writer, sheet_name='å¥å­è¯¦æƒ…', index=False)
            
            print(f"ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {excel_path}")
            
        except Exception as e:
            print(f"ç”ŸæˆExcelæŠ¥å‘Šå¤±è´¥: {str(e)}")

    def generate_visualizations(self, results, output_dir, timestamp):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            valid_results = [r for r in results if 'error' not in r]
            if not valid_results:
                return
            
            # åˆ›å»º2x3çš„å­å›¾å¸ƒå±€
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('Markdownæ–‡ä»¶ä¿¡æ¯å¯†åº¦åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
            
            # 1. ä¿¡æ¯å¯†åº¦åˆ†å¸ƒ
            densities = [r['average_density'] for r in valid_results]
            axes[0, 0].hist(densities, bins=20, alpha=0.7, color='lightblue', edgecolor='black')
            axes[0, 0].set_title('ä¿¡æ¯å¯†åº¦åˆ†å¸ƒ')
            axes[0, 0].set_xlabel('å¹³å‡ä¿¡æ¯å¯†åº¦')
            axes[0, 0].set_ylabel('æ–‡ä»¶æ•°é‡')
            axes[0, 0].axvline(x=0.12, color='red', linestyle='--', label='é«˜è´¨é‡é˜ˆå€¼')
            axes[0, 0].axvline(x=0.06, color='orange', linestyle='--', label='ä¸­è´¨é‡é˜ˆå€¼')
            axes[0, 0].legend()
            
            # 2. Markdownå…ƒç´ åˆ†å¸ƒ
            element_counts = {
                'æ ‡é¢˜': sum(r['markdown_stats']['headings_count'] for r in valid_results),
                'ä»£ç å—': sum(r['markdown_stats']['code_blocks_count'] for r in valid_results),
                'é“¾æ¥': sum(r['markdown_stats']['links_count'] for r in valid_results),
                'åˆ—è¡¨': sum(r['markdown_stats']['list_items_count'] for r in valid_results),
                'å›¾ç‰‡': sum(r['markdown_stats']['images_count'] for r in valid_results)
            }
            
            axes[0, 1].bar(element_counts.keys(), element_counts.values(), 
                          color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc'])
            axes[0, 1].set_title('Markdownå…ƒç´ åˆ†å¸ƒ')
            axes[0, 1].set_ylabel('æ•°é‡')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # 3. è´¨é‡åˆ†å¸ƒé¥¼å›¾
            quality_counts = {'é«˜': 0, 'ä¸­': 0, 'ä½': 0}
            for result in valid_results:
                for quality, count in result['quality_distribution'].items():
                    quality_counts[quality] += count
            
            colors = ['#ff9999', '#66b3ff', '#99ff99']
            axes[0, 2].pie(quality_counts.values(), labels=quality_counts.keys(), 
                          autopct='%1.1f%%', colors=colors)
            axes[0, 2].set_title('å¥å­è´¨é‡åˆ†å¸ƒ')
            
            # 4. æ–‡ä»¶è´¨é‡æ’å
            sorted_results = sorted(valid_results, key=lambda x: x['average_density'], reverse=True)
            top_10 = sorted_results[:10]
            filenames = [os.path.basename(r['filename'])[:20] + '...' 
                        if len(os.path.basename(r['filename'])) > 20 
                        else os.path.basename(r['filename']) for r in top_10]
            densities_top = [r['average_density'] for r in top_10]
            
            bars = axes[1, 0].barh(range(len(filenames)), densities_top, color='lightgreen')
            axes[1, 0].set_yticks(range(len(filenames)))
            axes[1, 0].set_yticklabels(filenames)
            axes[1, 0].set_xlabel('å¹³å‡ä¿¡æ¯å¯†åº¦')
            axes[1, 0].set_title('æ–‡ä»¶è´¨é‡æ’å (Top 10)')
            
            # 5. ç»“æ„åŒ–ç¨‹åº¦ vs ä¿¡æ¯å¯†åº¦
            structure_ratios = [r['markdown_stats']['structure_ratio'] for r in valid_results]
            axes[1, 1].scatter(structure_ratios, densities, alpha=0.6, color='purple')
            axes[1, 1].set_xlabel('ç»“æ„åŒ–ç¨‹åº¦')
            axes[1, 1].set_ylabel('ä¿¡æ¯å¯†åº¦')
            axes[1, 1].set_title('ç»“æ„åŒ–ç¨‹åº¦ vs ä¿¡æ¯å¯†åº¦')
            
            # 6. ä»£ç å¯†åº¦åˆ†å¸ƒ
            code_densities = [r['markdown_stats']['code_density'] for r in valid_results]
            axes[1, 2].hist(code_densities, bins=15, alpha=0.7, color='orange', edgecolor='black')
            axes[1, 2].set_title('ä»£ç å¯†åº¦åˆ†å¸ƒ')
            axes[1, 2].set_xlabel('ä»£ç å¯†åº¦')
            axes[1, 2].set_ylabel('æ–‡ä»¶æ•°é‡')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(output_dir, f"Markdownåˆ†æå›¾è¡¨_{timestamp}.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            # ç”Ÿæˆé¢å¤–çš„ä¸“é¡¹åˆ†æå›¾è¡¨
            self.generate_additional_charts(valid_results, output_dir, timestamp)
            
            print(f"å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ: {chart_path}")
            
        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {str(e)}")

    def generate_additional_charts(self, results, output_dir, timestamp):
        """ç”Ÿæˆé¢å¤–çš„ä¸“é¡¹åˆ†æå›¾è¡¨"""
        try:
            # åˆ›å»ºæ ‡é¢˜å±‚çº§åˆ†æå›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Markdownæ–‡æ¡£ç»“æ„æ·±åº¦åˆ†æ', fontsize=14, fontweight='bold')
            
            # 1. æ ‡é¢˜å±‚çº§åˆ†å¸ƒ
            all_heading_levels = defaultdict(int)
            for result in results:
                heading_levels = result['markdown_stats']['heading_levels']
                for level, count in heading_levels.items():
                    all_heading_levels[level] += count
            
            if all_heading_levels:
                levels = sorted(all_heading_levels.keys())
                counts = [all_heading_levels[level] for level in levels]
                axes[0, 0].bar(levels, counts, color='skyblue')
                axes[0, 0].set_title('æ ‡é¢˜å±‚çº§åˆ†å¸ƒ')
                axes[0, 0].set_xlabel('æ ‡é¢˜å±‚çº§')
                axes[0, 0].set_ylabel('æ•°é‡')
            
            # 2. æ–‡æ¡£é•¿åº¦ vs è´¨é‡å…³ç³»
            doc_lengths = [r['total_chars'] for r in results]
            quality_ratios = [r['high_quality_ratio'] for r in results]
            
            scatter = axes[0, 1].scatter(doc_lengths, quality_ratios, 
                                       c=[r['average_density'] for r in results], 
                                       cmap='viridis', alpha=0.6)
            axes[0, 1].set_xlabel('æ–‡æ¡£é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰')
            axes[0, 1].set_ylabel('é«˜è´¨é‡å¥å­å æ¯”')
            axes[0, 1].set_title('æ–‡æ¡£é•¿åº¦ vs è´¨é‡å…³ç³»')
            plt.colorbar(scatter, ax=axes[0, 1], label='å¹³å‡ä¿¡æ¯å¯†åº¦')
            
            # 3. ä¸åŒå…ƒç´ ç±»å‹çš„å¯†åº¦è´¡çŒ®
            element_contributions = {
                'æ•°å­—ä¿¡æ¯': sum(r['content_stats']['numbers_count'] for r in results),
                'ä¸“ä¸šæœ¯è¯­': sum(r['content_stats']['professional_terms_count'] for r in results),
                'ä»£ç ç‰‡æ®µ': sum(r['content_stats']['code_snippets_count'] for r in results),
                'é“¾æ¥': sum(r['content_stats']['links_count'] for r in results),
                'æ—¶é—´è¯': sum(r['content_stats']['time_words_count'] for r in results)
            }
            
            axes[1, 0].pie(element_contributions.values(), labels=element_contributions.keys(),
                          autopct='%1.1f%%', startangle=90)
            axes[1, 0].set_title('ä¿¡æ¯å¯†åº¦è´¡çŒ®åˆ†å¸ƒ')
            
            # 4. æ–‡æ¡£ç»“æ„å¤æ‚åº¦åˆ†æ
            complexity_scores = []
            filenames = []
            
            for result in results:
                md_stats = result['markdown_stats']
                # è®¡ç®—å¤æ‚åº¦å¾—åˆ†
                complexity = (
                    md_stats['headings_count'] * 0.3 +
                    md_stats['code_blocks_count'] * 0.4 +
                    md_stats['tables_count'] * 0.5 +
                    md_stats['list_items_count'] * 0.2 +
                    md_stats['links_count'] * 0.1
                ) / md_stats['content_lines'] if md_stats['content_lines'] > 0 else 0
                
                complexity_scores.append(complexity)
                filenames.append(os.path.basename(result['filename'])[:15])
            
            # æ˜¾ç¤ºå¤æ‚åº¦æœ€é«˜çš„10ä¸ªæ–‡æ¡£
            sorted_data = sorted(zip(complexity_scores, filenames), reverse=True)[:10]
            if sorted_data:
                scores, names = zip(*sorted_data)
                y_pos = range(len(names))
                
                axes[1, 1].barh(y_pos, scores, color='coral')
                axes[1, 1].set_yticks(y_pos)
                axes[1, 1].set_yticklabels(names)
                axes[1, 1].set_xlabel('ç»“æ„å¤æ‚åº¦å¾—åˆ†')
                axes[1, 1].set_title('æ–‡æ¡£ç»“æ„å¤æ‚åº¦æ’å')
            
            plt.tight_layout()
            
            additional_chart_path = os.path.join(output_dir, f"Markdownæ·±åº¦åˆ†æ_{timestamp}.png")
            plt.savefig(additional_chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"ç”Ÿæˆé¢å¤–å›¾è¡¨å¤±è´¥: {str(e)}")

    def generate_json_report(self, results, failed_files, output_dir, timestamp):
        """ç”ŸæˆJSONè¯¦ç»†æŠ¥å‘Š"""
        try:
            report_data = {
                'analysis_time': datetime.now().isoformat(),
                'summary': {
                    'total_files': len(results) + len(failed_files),
                    'successful_files': len(results),
                    'failed_files': len(failed_files),
                    'analyzer_version': '1.0.0',
                    'analysis_type': 'Markdownä¿¡æ¯å¯†åº¦åˆ†æ'
                },
                'global_stats': self.calculate_global_stats(results),
                'results': results,
                'failed_files': failed_files,
                'analysis_config': {
                    'weights': self.weights,
                    'quality_thresholds': {
                        'high': 0.12,
                        'medium': 0.06
                    }
                }
            }
            
            json_path = os.path.join(output_dir, f"Markdownè¯¦ç»†æŠ¥å‘Š_{timestamp}.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"JSONè¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")
            
        except Exception as e:
            print(f"ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {str(e)}")

    def calculate_global_stats(self, results):
        """è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
        valid_results = [r for r in results if 'error' not in r]
        if not valid_results:
            return {}
        
        total_sentences = sum(r['total_sentences'] for r in valid_results)
        total_chars = sum(r['total_chars'] for r in valid_results)
        total_high_quality = sum(r['quality_distribution']['é«˜'] for r in valid_results)
        
        # Markdownç‰¹æœ‰ç»Ÿè®¡
        total_headings = sum(r['markdown_stats']['headings_count'] for r in valid_results)
        total_code_blocks = sum(r['markdown_stats']['code_blocks_count'] for r in valid_results)
        total_links = sum(r['markdown_stats']['links_count'] for r in valid_results)
        total_images = sum(r['markdown_stats']['images_count'] for r in valid_results)
        
        return {
            'total_sentences': total_sentences,
            'total_characters': total_chars,
            'average_density': sum(r['average_density'] for r in valid_results) / len(valid_results),
            'global_high_quality_ratio': total_high_quality / total_sentences if total_sentences > 0 else 0,
            'markdown_elements': {
                'total_headings': total_headings,
                'total_code_blocks': total_code_blocks,
                'total_links': total_links,
                'total_images': total_images,
                'avg_headings_per_doc': total_headings / len(valid_results),
                'avg_code_blocks_per_doc': total_code_blocks / len(valid_results)
            },
            'content_analysis': {
                'total_professional_terms': sum(r['content_stats']['professional_terms_count'] for r in valid_results),
                'total_numbers': sum(r['content_stats']['numbers_count'] for r in valid_results),
                'total_time_words': sum(r['content_stats']['time_words_count'] for r in valid_results),
                'avg_sentence_length': sum(r['content_stats']['avg_sentence_length'] for r in valid_results) / len(valid_results)
            }
        }

    def generate_summary_report(self, results, failed_files, output_dir, timestamp):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š"""
        try:
            valid_results = [r for r in results if 'error' not in r]
            global_stats = self.calculate_global_stats(results)
            
            report_lines = [
                "=" * 70,
                "Markdownæ–‡ä»¶ä¿¡æ¯å¯†åº¦åˆ†ææ‘˜è¦æŠ¥å‘Š",
                "=" * 70,
                f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"åˆ†æå·¥å…·: Markdownä¿¡æ¯å¯†åº¦åˆ†æå™¨ v1.0.0",
                "",
                "ğŸ“Š æ–‡ä»¶ç»Ÿè®¡:",
                "-" * 40,
                f"æ€»æ–‡ä»¶æ•°: {len(results) + len(failed_files)}",
                f"æˆåŠŸåˆ†æ: {len(valid_results)} ä¸ª",
                f"å¤±è´¥æ–‡ä»¶: {len(failed_files)} ä¸ª",
                f"æˆåŠŸç‡: {len(valid_results)/(len(results) + len(failed_files))*100:.1f}%",
                "",
                "ğŸ“ˆ æ•´ä½“è´¨é‡æŒ‡æ ‡:",
                "-" * 40
            ]
            
            if valid_results:
                report_lines.extend([
                    f"æ€»å¥å­æ•°: {global_stats['total_sentences']:,}",
                    f"æ€»å­—ç¬¦æ•°: {global_stats['total_characters']:,}",
                    f"å¹³å‡ä¿¡æ¯å¯†åº¦: {global_stats['average_density']:.4f}",
                    f"å…¨å±€é«˜è´¨é‡å¥å­å æ¯”: {global_stats['global_high_quality_ratio']:.2%}",
                    f"å¹³å‡å¥å­é•¿åº¦: {global_stats['content_analysis']['avg_sentence_length']:.1f} å­—ç¬¦",
                    "",
                    "ğŸ“ Markdownå…ƒç´ ç»Ÿè®¡:",
                    "-" * 40,
                    f"æ ‡é¢˜æ€»æ•°: {global_stats['markdown_elements']['total_headings']:,}",
                    f"ä»£ç å—æ€»æ•°: {global_stats['markdown_elements']['total_code_blocks']:,}",
                    f"é“¾æ¥æ€»æ•°: {global_stats['markdown_elements']['total_links']:,}",
                    f"å›¾ç‰‡æ€»æ•°: {global_stats['markdown_elements']['total_images']:,}",
                    f"å¹³å‡æ¯æ–‡æ¡£æ ‡é¢˜æ•°: {global_stats['markdown_elements']['avg_headings_per_doc']:.1f}",
                    f"å¹³å‡æ¯æ–‡æ¡£ä»£ç å—æ•°: {global_stats['markdown_elements']['avg_code_blocks_per_doc']:.1f}",
                    "",
                    "ğŸ¯ å†…å®¹åˆ†æ:",
                    "-" * 40,
                    f"ä¸“ä¸šæœ¯è¯­æ€»æ•°: {global_stats['content_analysis']['total_professional_terms']:,}",
                    f"æ•°å­—ä¿¡æ¯æ€»æ•°: {global_stats['content_analysis']['total_numbers']:,}",
                    f"æ—¶é—´è¯æ€»æ•°: {global_stats['content_analysis']['total_time_words']:,}",
                    "",
                    "ğŸ† æ–‡ä»¶è´¨é‡æ’å (æŒ‰ä¿¡æ¯å¯†åº¦):",
                    "-" * 40
                ])
                
                # æ’åºå¹¶æ˜¾ç¤ºå‰15å
                sorted_results = sorted(valid_results, key=lambda x: x['average_density'], reverse=True)
                for i, result in enumerate(sorted_results[:15], 1):
                    filename = os.path.basename(result['filename'])
                    report_lines.append(
                        f"{i:2d}. {filename:<35} "
                        f"å¯†åº¦: {result['average_density']:.4f} "
                        f"é«˜è´¨é‡: {result['high_quality_ratio']:.1%} "
                        f"æ ‡é¢˜: {result['markdown_stats']['headings_count']:2d}"
                    )
                
                # è´¨é‡åˆ†å¸ƒç»Ÿè®¡
                report_lines.extend([
                    "",
                    "ğŸ“Š è´¨é‡åˆ†å¸ƒç»Ÿè®¡:",
                    "-" * 40
                ])
                
                quality_totals = {'é«˜': 0, 'ä¸­': 0, 'ä½': 0}
                for result in valid_results:
                    for quality, count in result['quality_distribution'].items():
                        quality_totals[quality] += count
                
                total_quality_sentences = sum(quality_totals.values())
                for quality, count in quality_totals.items():
                    ratio = count / total_quality_sentences if total_quality_sentences > 0 else 0
                    report_lines.append(f"{quality}è´¨é‡å¥å­: {count:,} ({ratio:.1%})")
                
                # ç»“æ„åˆ†æ
                report_lines.extend([
                    "",
                    "ğŸ—ï¸ æ–‡æ¡£ç»“æ„åˆ†æ:",
                    "-" * 40
                ])
                
                avg_structure_ratio = sum(r['markdown_stats']['structure_ratio'] for r in valid_results) / len(valid_results)
                avg_code_density = sum(r['markdown_stats']['code_density'] for r in valid_results) / len(valid_results)
                
                report_lines.extend([
                    f"å¹³å‡ç»“æ„åŒ–ç¨‹åº¦: {avg_structure_ratio:.2%}",
                    f"å¹³å‡ä»£ç å¯†åº¦: {avg_code_density:.2%}",
                ])
                
                # æœ€ä½³å®è·µå»ºè®®
                report_lines.extend([
                    "",
                    "ğŸ’¡ ä¼˜åŒ–å»ºè®®:",
                    "-" * 40
                ])
                
                low_quality_files = [r for r in valid_results if r['average_density'] < 0.06]
                high_structure_files = [r for r in valid_results if r['markdown_stats']['structure_ratio'] > 0.3]
                
                if low_quality_files:
                    report_lines.append(f"â€¢ {len(low_quality_files)} ä¸ªæ–‡ä»¶ä¿¡æ¯å¯†åº¦è¾ƒä½ï¼Œå»ºè®®å¢åŠ å…·ä½“æ•°æ®å’Œä¸“ä¸šæœ¯è¯­")
                
                if len(high_structure_files) / len(valid_results) < 0.5:
                    report_lines.append("â€¢ å»ºè®®å¢åŠ æ–‡æ¡£ç»“æ„åŒ–å…ƒç´ ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰")
                
                if global_stats['markdown_elements']['avg_code_blocks_per_doc'] < 1:
                    report_lines.append("â€¢ æŠ€æœ¯æ–‡æ¡£å»ºè®®å¢åŠ ä»£ç ç¤ºä¾‹ä»¥æé«˜å®ç”¨æ€§")
                
                # æ ‡é¢˜å±‚çº§å»ºè®®
                heading_distribution = defaultdict(int)
                for result in valid_results:
                    for level, count in result['markdown_stats']['heading_levels'].items():
                        heading_distribution[level] += count
                
                if 'h1' in heading_distribution and heading_distribution['h1'] > len(valid_results):
                    report_lines.append("â€¢ å»ºè®®å‡å°‘ä¸€çº§æ ‡é¢˜ä½¿ç”¨ï¼Œä¿æŒæ–‡æ¡£å±‚æ¬¡æ¸…æ™°")
                
                if sum(heading_distribution.values()) / len(valid_results) < 3:
                    report_lines.append("â€¢ å»ºè®®å¢åŠ æ ‡é¢˜æ•°é‡ï¼Œæ”¹å–„æ–‡æ¡£å¯è¯»æ€§")
            
            # å¤±è´¥æ–‡ä»¶åˆ—è¡¨
            if failed_files:
                report_lines.extend([
                    "",
                    "âŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:",
                    "-" * 40
                ])
                for failed in failed_files:
                    report_lines.append(f"- {failed['filename']}: {failed['error']}")
            
            # æŠ€æœ¯è¯´æ˜
            report_lines.extend([
                "",
                "ğŸ”§ æŠ€æœ¯è¯´æ˜:",
                "-" * 40,
                "â€¢ ä¿¡æ¯å¯†åº¦ = ä¿¡æ¯ä»·å€¼å¾—åˆ† / å­—ç¬¦æ•°",
                "â€¢ é«˜è´¨é‡é˜ˆå€¼: 0.12ï¼Œä¸­è´¨é‡é˜ˆå€¼: 0.06",
                "â€¢ æƒé‡è®¾ç½®: æ•°å­—(3.0), æ—¶é—´(2.5), ä»£ç (2.5), ä¸“ä¸šæœ¯è¯­(2.0)",
                "â€¢ ç»“æ„åŒ–ç¨‹åº¦ = (æ ‡é¢˜+åˆ—è¡¨+è¡¨æ ¼+å¼•ç”¨) / å†…å®¹è¡Œæ•°",
                "â€¢ ä»£ç å¯†åº¦ = (ä»£ç å—+å†…è”ä»£ç ) / å†…å®¹è¡Œæ•°",
                "",
                "=" * 70,
                "æŠ¥å‘Šç»“æŸ - æ„Ÿè°¢ä½¿ç”¨Markdownä¿¡æ¯å¯†åº¦åˆ†æå·¥å…·"
            ])
            
            # ä¿å­˜æŠ¥å‘Š
            summary_path = os.path.join(output_dir, f"Markdownæ‘˜è¦æŠ¥å‘Š_{timestamp}.txt")
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            
            # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
            print("\n" + '\n'.join(report_lines))
            print(f"\næ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
            
        except Exception as e:
            print(f"ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("Markdownæ–‡ä»¶ä¿¡æ¯å¯†åº¦æ‰¹é‡åˆ†æå·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    missing_packages = []
    try:
        import jieba
    except ImportError:
        missing_packages.append('jieba')
    
    try:
        import pandas
    except ImportError:
        missing_packages.append('pandas')
    
    try:
        import matplotlib
    except ImportError:
        missing_packages.append('matplotlib')
    
    try:
        import seaborn
    except ImportError:
        missing_packages.append('seaborn')
    
    try:
        import markdown
    except ImportError:
        missing_packages.append('markdown')
    
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        missing_packages.append('beautifulsoup4')
    
    try:
        import yaml
    except ImportError:
        missing_packages.append('PyYAML')
    
    try:
        import openpyxl
    except ImportError:
        missing_packages.append('openpyxl')
    
    if missing_packages:
        print("è¯·å…ˆå®‰è£…å¿…è¦çš„åº“:")
        print(f"pip install {' '.join(missing_packages)}")
        return
    
    # è·å–ç”¨æˆ·è¾“å…¥
    while True:
        folder_path = input("è¯·è¾“å…¥Markdownæ–‡ä»¶å¤¹è·¯å¾„: ").strip().strip('"')
        if os.path.exists(folder_path):
            break
        print("æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    output_dir = input("è¯·è¾“å…¥è¾“å‡ºç›®å½•è·¯å¾„ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„ 'markdown_analysis_results'): ").strip().strip('"')
    if not output_dir:
        output_dir = "markdown_analysis_results"
    
    print(f"\nå¼€å§‹åˆ†æ {folder_path} ä¸­çš„Markdownæ–‡ä»¶...")
    print("æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å: .md, .markdown, .mdown, .mkd")
    print("-" * 60)
    
    # åˆ›å»ºåˆ†æå™¨å¹¶å¼€å§‹åˆ†æ
    analyzer = MarkdownInfoDensityAnalyzer()
    analyzer.analyze_markdown_folder(folder_path, output_dir)

if __name__ == "__main__":
    main()
